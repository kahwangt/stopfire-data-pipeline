{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import datetime as dt\n",
    "import pygeohash as gh\n",
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_assignment_db\n",
    "    taskc = db.taskc\n",
    "    # store arrival time for current batch in a variable processing_time\n",
    "    arrival_time= dt.datetime.now().strftime(\"%X\")\n",
    "    for record in iter:\n",
    "        data = json.loads(record[1])\n",
    "        # initialize empty dict to store climate, hotspot data\n",
    "        climate = {}\n",
    "        hotspot = {}\n",
    "        # if data is from producer 1 which is the climate data\n",
    "        if data.get(\"sender_id\") == 1:\n",
    "            # extract the attributes for this climate record\n",
    "            climate[\"_id\"] = data.get(\"created_time\")\n",
    "            climate[\"station_latitude\"] = float(data.get(\"latitude\"))\n",
    "            climate[\"station_longitude\"] = float(data.get(\"longitude\"))\n",
    "            climate[\"air_temperature_celcius\"] = int(data.get(\"air_temperature_celcius\"))\n",
    "            climate[\"relative_humidity\"] = float(data.get(\"relative_humidity\"))\n",
    "            climate[\"windspeed_knots\"] = float(data.get(\"windspeed_knots\"))\n",
    "            climate[\"max_wind_speed\"] = float(data.get(\"max_wind_speed\"))\n",
    "            climate[\"precipitation\"] = data.get(\"precipitation\")\n",
    "            climate[\"hotspots\"] = []\n",
    "            climate[\"arrival_time\"] = arrival_time\n",
    "            # write this climate record to MongoDB\n",
    "            try:\n",
    "                taskc.replace_one({\"_id\":data.get(\"created_time\")}, climate, True)\n",
    "            except Exception as ex:\n",
    "                print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "        # if data is from producer 2 or producer 3 which is the hotspot data\n",
    "        elif (data.get(\"sender_id\") == 2 or data.get(\"sender_id\") == 3):\n",
    "            # initialize empty dict to store geohash of current batch climate records\n",
    "            geohash = {}\n",
    "            # get latitude and longitude of this current hotspot record\n",
    "            latitude = float(data.get(\"latitude\"))\n",
    "            longitude = float(data.get(\"longitude\"))\n",
    "            # calculate the geohash string representation of this hotspot with precision 5\n",
    "            hotspot_geohash = gh.encode(latitude, longitude, precision=5)\n",
    "            # find the climate data for this batch using the arrival time and extract it's id, lat, lon\n",
    "            cursor = taskc.find({\"arrival_time\": arrival_time},{\\\n",
    "                            \"_id\":1,\"station_latitude\":1,\"station_longitude\":1})\n",
    "            # for each climate record stored in this batch, we calculate it's geohash representation \n",
    "            for each in cursor: \n",
    "                geohash[each['_id']] = gh.encode(each['station_latitude'],each['station_longitude'],precision=5)\n",
    "            # compare hotspot_geohash with the climate's geohash and get id of climate if geohash matches\n",
    "            for key, value in geohash.items():\n",
    "                if hotspot_geohash in value:\n",
    "                    assigned_climate = key\n",
    "            # process this hotspot record only if this hotspot matches the climate geohash\n",
    "            if len(assigned_climate) > 0:\n",
    "                # extract the attributes for this hotspot record\n",
    "                hotspot[\"created_time\"] = data.get(\"created_time\")\n",
    "                hotspot[\"hotspot_latitude\"] = latitude\n",
    "                hotspot[\"hotspot_longitude\"] = longitude\n",
    "                # if there is no hotspot record tagged under this climate's record, update hotspot record\n",
    "                if (taskc.find({\"_id\": assigned_climate, \"hotspots\": {'$not': {'$size':0}}}).count()) == 0:\n",
    "                    hotspot[\"confidence\"] = int(data.get(\"confidence\"))\n",
    "                    hotspot[\"surface_temperature_celcius\"] = int(data.get(\"surface_temperature_celcius\"))\n",
    "                    try:\n",
    "                        taskc.update_one({'_id': assigned_climate},{'$push':{\"hotspots\":hotspot}})\n",
    "                    except Exception as ex:\n",
    "                        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                # if there is a hotspot record under the climate's record,\n",
    "                # we will need to average the surface temperature and confidence\n",
    "                elif (taskc.find({\"_id\": assigned_climate, \"hotspots\": {'$not': {'$size':0}}}).count()) > 0:\n",
    "                    # store confidence and surface temp of current hotspot record\n",
    "                    confidence_1 = int(data.get(\"confidence\"))\n",
    "                    sur_temp_cel_1 = int(data.get(\"surface_temperature_celcius\"))\n",
    "                    # find the confidence and surface temp of hotspot stored under this climate\n",
    "                    cursor = taskc.find({\"_id\": assigned_climate} ,{\"_id\":0, \"hotspots.confidence\":1,\\\n",
    "                                                               \"hotspots.surface_temperature_celcius\":1})\n",
    "                    # extract the confidence and surface temp\n",
    "                    for each in cursor:\n",
    "                        confidence_2 = each['hotspots'][0]['confidence']\n",
    "                        sur_temp_cel_2 = each['hotspots'][0]['surface_temperature_celcius']\n",
    "                    # calculate the average of confidence and surface temperature\n",
    "                    avg_confidence = (confidence_1 + confidence_2)/2\n",
    "                    avg_sur_temp_cel = (sur_temp_cel_1 + sur_temp_cel_2)/2\n",
    "                    # assign confidence and surface temp of current hotspot record to the average\n",
    "                    hotspot['confidence'] = avg_confidence\n",
    "                    hotspot['surface_temperature_celcius'] = avg_sur_temp_cel\n",
    "                    try:\n",
    "                        # update the confidence and surface temp of previously stored hotspot record\n",
    "                        taskc.update_one({'_id': assigned_climate, \"hotspots.confidence\": confidence_2},{\\\n",
    "                                            '$set':{\"hotspots.$.confidence\":avg_confidence}})\n",
    "                        taskc.update_one({'_id': assigned_climate, \"hotspots.surface_temperature_celcius\":\\\n",
    "                                          sur_temp_cel_2},{'$set':{\"hotspots.$.surface_temperature_celcius\":\\\n",
    "                                                                    avg_sur_temp_cel}})\n",
    "                        # push this hotspot record into the climate that has same geohash\n",
    "                        taskc.update_one({'_id': assigned_climate},{'$push':{\"hotspots\":hotspot}})\n",
    "                    except Exception as ex:\n",
    "                        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                    \n",
    "    client.close()\n",
    "\n",
    "# Create a local StreamingContext with as many working processors as possible and a batch interval of 10 seconds            \n",
    "batch_interval = 10\n",
    "\n",
    "# local[2]: run Spark locally with 2 execution threads\n",
    "conf = SparkConf().setAppName(\"ReportFire\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "# entry point for spark streaming, a batch interval of 10 seconds \n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "topic_1 = \"climate_data\"\n",
    "topic_2 = \"hotspot_data\"\n",
    "\n",
    "kafkaStream_climate = KafkaUtils.createDirectStream(ssc, [topic_1], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092',\n",
    "                        'group.id':'reportfire-group',\n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "\n",
    "kafkaStream_hotspot = KafkaUtils.createDirectStream(ssc, [topic_2], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092',\n",
    "                        'group.id':'reportfire-group',\n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "\n",
    "\n",
    "# for every batch, send the two RDDs and all partitions to the function 'sendDataToDB'\n",
    "climate = kafkaStream_climate.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "hotspot = kafkaStream_hotspot.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(3600) # Run stream for 60 minutes\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
